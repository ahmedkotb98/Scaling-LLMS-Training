{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing DeepSpeed: A Hands-On Approach üöÄ\n",
        "\n",
        "## Learning Objectives üéØ\n",
        "- Learn how to implement DeepSpeed to optimize training for large models.\n",
        "- Understand the configuration of DeepSpeed for memory-efficient training.\n",
        "- Gain hands-on experience with DeepSpeed, even on a single GPU, to grasp the key concepts."
      ],
      "metadata": {
        "id": "GrzSluAyj1F-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Verification ‚úÖ\n",
        "Verify that a GPU is available. While a T4 (free tier) GPU cannot handle the original 8B model, we will use a smaller 1.1B model for faster loading and to fit within Colab limits.\n",
        "\n",
        "For those with multiple GPUs, DeepSpeed will show significant performance gains."
      ],
      "metadata": {
        "id": "ArNV-jB37Cb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check so there is a gpu available, a T4(free tier) will NOT be enough for an 8B parameter model, but we can use a slightly smaller one and the lesson will remain the same\n",
        "assert (torch.cuda.is_available()==True)"
      ],
      "metadata": {
        "id": "7sGUU2WWKiTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Axolotl and DeepSpeed üõ†Ô∏è\n",
        "Install Axolotl with DeepSpeed support. While DeepSpeed is optimized for multi-GPU setups, you can still run this configuration on a single GPU to understand how the system works and the benefits of the Zero Redundancy Optimizer (ZeRO)."
      ],
      "metadata": {
        "id": "Supg6kJ-7KE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e 'git+https://github.com/axolotl-ai-cloud/axolotl.git@0aeb277#egg=axolotl[deepspeed]' # ensures the same version we used in the course"
      ],
      "metadata": {
        "id": "qTRnNMOSj6Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration Setup for DeepSpeed Training üìù\n",
        "Set up a YAML configuration for training the model, using a smaller model for free-tier compatibility. The original model from the lesson can be used if you have access to more powerful hardware."
      ],
      "metadata": {
        "id": "lLQmWq0E7QBz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zrTS3fvjvW7"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "train_config = {\n",
        "    # \"base_model\": \"unsloth/Meta-Llama-3.1-8B-Instruct\", # The original model from the lesson\n",
        "    \"base_model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", # For faster loading on Colab\n",
        "\n",
        "\n",
        "    # dataset params\n",
        "    \"datasets\": [\n",
        "        {\n",
        "            \"path\": \"TheFuzzyScientist/squad-for-llms\",\n",
        "            \"type\": {\n",
        "                \"system_prompt\": \"Read the following context and concisely answer my question.\",\n",
        "                \"field_system\": \"system\",\n",
        "                \"field_instruction\": \"question\",\n",
        "                \"field_input\": \"context\",\n",
        "                \"field_output\": \"output\",\n",
        "                \"format\": \"<|user|> {input} {instruction} </s> <|assistant|>\",\n",
        "                \"no_input_format\": \"<|user|> {instruction} </s> <|assistant|>\",\n",
        "            },\n",
        "        }\n",
        "    ],\n",
        "    \"output_dir\": \"./models/Llama3_squad\",\n",
        "\n",
        "    # model params\n",
        "    \"sequence_length\": 2048,\n",
        "    \"bf16\": \"auto\",\n",
        "    \"tf32\": False,\n",
        "\n",
        "    # training params\n",
        "    \"micro_batch_size\": 4,\n",
        "    \"num_epochs\": 4,\n",
        "    \"optimizer\": \"adamw_bnb_8bit\",\n",
        "    \"learning_rate\": 0.0002,\n",
        "    \"logging_steps\": 1,\n",
        "\n",
        "    # LoRA / qLoRA\n",
        "    \"adapter\": \"lora\",\n",
        "    \"lora_r\": 32,\n",
        "    \"lora_alpha\": 16,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"lora_target_linear\": True,\n",
        "\n",
        "    # Gradient Accumulation\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "\n",
        "    # Gradient Checkpointing\n",
        "    \"gradient_checkpointing\": True,\n",
        "}\n",
        "\n",
        "\n",
        "# Write the YAML file\n",
        "with open(\"deepspeed_train.yml\", 'w') as file:\n",
        "    yaml.dump(train_config, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepSpeed Configuration üß†\n",
        "Create a DeepSpeed configuration (Zero Stage 1) to enable memory optimization during training. This configuration will reduce memory usage and allow larger batch sizes, especially beneficial when scaling to multiple GPUs."
      ],
      "metadata": {
        "id": "9kzwXzfK7RLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "zero1_conf = {\n",
        "    \"zero_optimization\": {\"stage\": 1, \"overlap_comm\": True},\n",
        "    \"bf16\": {\"enabled\": \"auto\"},\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"auto_cast\": False,\n",
        "        \"loss_scale\": 0,\n",
        "        \"initial_scale_power\": 32,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1,\n",
        "    },\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": False,\n",
        "}\n",
        "\n",
        "with open(\"zero1.json\", 'w') as fp:\n",
        "  json.dump(zero1_conf, fp)"
      ],
      "metadata": {
        "id": "k2uNnukh4Fhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launching DeepSpeed Training üöÄ\n",
        "Launch the training using the `accelerate launch` command with DeepSpeed enabled. While running this on a single GPU won't show the full benefits, it will still provide the learning experience and understanding of how DeepSpeed optimizes large-scale training."
      ],
      "metadata": {
        "id": "kLh2l5gT7Sw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.train deepspeed_train.yml --deepspeed zero1.json"
      ],
      "metadata": {
        "id": "5WKK6Z0toM9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Merge the trained adapter\n",
        "!accelerate launch -m axolotl.cli.merge_lora deepspeed_train.yml"
      ],
      "metadata": {
        "id": "C4lRjYdihjbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "25YGeFyMiN05"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
